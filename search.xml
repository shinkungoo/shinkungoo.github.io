<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>统计因果推理学习 01</title>
    <url>/2022/09/20/CausalityLearning01/</url>
    <content><![CDATA[<p><img src="/img/Causality/cover.png"></p>
<p>本博客阅读书本为<em>Causality Inference in Statistics: A Primer</em>（中文译《统计因果推理入门》），作者Judea Pearl</p>
<hr>
<p>我们为什么去研究因果？这个问题，自己仅有比较模糊的认识。但在自己比较微薄的统计学中，隐约能够感觉到统计学中简单的相关性的局限性。就像这本书中所说</p>
<blockquote>
<p>“因果关系”这一概念说明了关于这个世界的一些信息，而实证的统计学方法却不能</p>
</blockquote>
<p>就比如，一个很简单的例子</p>
<h2 id="辛普森悖论（Simpson’s-paradox）"><a href="#辛普森悖论（Simpson’s-paradox）" class="headerlink" title="辛普森悖论（Simpson’s paradox）"></a>辛普森悖论（Simpson’s paradox）</h2><p>辛普森悖论指出<strong>存在这样的数据，在总体上的统计结果与其每一个子部分的统计结果相反</strong>，就比如这样的一个数据</p>
<p><img src="/img/Causality/0000.png" title="一种新药临床试验对比（考虑性别因素）"></p>
<p>我们发现这个表哥有个很荒谬的地方——虽然对于男性患者和女性患者分别而言，他们都是治愈的；但是整体来看，这个药物竟然有害。也就是说，如果我们知道性别就可以开药，反之，不知道性别时就不能开药。</p>
<p>这个问题无法从统计学中找到答案，但我们可以从因果中寻找原因。如果我们假设：雌性激素对患者的痊愈有负面作用。同时我们发现，女性患者更加偏向于选择用药。那么，<strong>性别是用药与痊愈的共同影响因素</strong>。也就是一种<strong>混淆因子</strong>，画成图也就是这样：</p>
<p><img src="/img/Causality/0001.png" title="上图的一个因果图"></p>
<p>而对于一个连续的例子，比如下图</p>
<p><img src="/img/Causality/0002.png" title="锻炼量-胆固醇研究结果（按年龄划分）"></p>
<p>这个研究可以发现，如果按照年龄分组，那么每组总体都再呈现下降趋势，但如果不分组，就会得到这样的结果</p>
<p><img src="/img/Causality/0003.png" title="锻炼量-胆固醇研究结果（不按年龄划分）"></p>
<p>这似乎表明越锻炼胆固醇越高，可是我们很容易知道老年人的胆固醇在相同的锻炼量下比年轻人高，说明年龄是与锻炼和胆固醇都相关的共同因素，因此如果不按照年龄分组，就会得到错误的结论。</p>
<p>所以，针对与辛普森悖论，我们可以<strong>固定辛普森悖论中的这个共同因素（混淆因子）来做出较为正确的决策</strong>。</p>
<h2 id="因果关系（Causality）"><a href="#因果关系（Causality）" class="headerlink" title="因果关系（Causality）"></a>因果关系（Causality）</h2><blockquote>
<p>如果变量$Y$的值以某种形式依赖于变量$X$的数值，那么变量$X$就是变量$Y$的原因。</p>
</blockquote>
<p>由前面的例子可以得知，仅仅根据数据来确定因果关系是不确切的，因此统计方法在一些方面不能支撑决策。而我们要过渡到”去描述数据中的因果关系”，还需要完成这些事情：</p>
<ul>
<li>给出”因果关系”可操作性的定义</li>
<li>给出因果假设的形式化的方法，即建立因果模型</li>
<li>给出因果模型结构与数据特征相联系的方法</li>
<li>给出从数据与模型的因果假设中得出结论的方法</li>
</ul>
<h2 id="概率和统计（Probability-and-Statistics）"><a href="#概率和统计（Probability-and-Statistics）" class="headerlink" title="概率和统计（Probability and Statistics）"></a>概率和统计（Probability and Statistics）</h2><p>因为我们需要用数学的语言去描述因果，所以一些常用的数学知识是必要的。</p>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><blockquote>
<p>变量是能取多个值的任意属性或符号，变量$X$所取值$x$的概率记作$P(X&#x3D;x)$，没有歧义的时候记作$P(x)$。变量可以分为离散变量和连续变量。</p>
</blockquote>
<h3 id="事件"><a href="#事件" class="headerlink" title="事件"></a>事件</h3><blockquote>
<p>为一个变量或者一个变量集合指定一个值或者一组值，称为一个事件。</p>
</blockquote>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>假设已知事件$B$已经发生，那么此时事件$A$发生的概率称为在$B$条件下$A$的条件概率。已知$Y&#x3D;y$条件下，$X&#x3D;x$的条件概率记作$P(X&#x3D;x|Y&#x3D;y)$，通常记作$P(x|y)$</p>
<p>同时也存在这样的情况：一个事件的概率不随着另外的事件发生改变，这也称之为<strong>独立性</strong>。此时就会有</p>
<p>$$<br>P(A|B)&#x3D;P(A)<br>$$</p>
<p>并且一定会有<br>$$<br>P(A|B)&#x3D;P(A) \Leftrightarrow P(B|A)&#x3D;P(B)<br>$$</p>
<p>给定第三个事件$C$，如果</p>
<p>$$<br>P(A|BC)&#x3D;P(A|C) \Leftrightarrow P(B|AC)&#x3D;P(B|C)<br>$$</p>
<p>那么也可以说明$A,B$两个事件相互独立。</p>
<p>变量与事件相似，也可以这样表示独立性，即</p>
<p>$$<br>P(X&#x3D;x|Y&#x3D;y)&#x3D;P(X&#x3D;x) \Leftrightarrow P(Y&#x3D;y|X&#x3D;x)&#x3D;P(Y&#x3D;y)<br>$$</p>
<h3 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h3><p>变量$X$的概率分布是$X$的每一个可能取值的概率的集合。它可以是离散的，也可以是连续的。连续的表示$f$满足$\int_{-\infty }^{\infty }f(x)dx &#x3D; 1$</p>
<h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>首先，对于任何两个互斥事件$A,B$（即$A,B$不能同时发生），则有</p>
<p>$$<br>P(A+B)&#x3D;P(A)+P(B)<br>$$</p>
<p>由此，对于这样两个事件$A,B$，有</p>
<p>$$<br>P(A)&#x3D;P(AB)+P(A \overline{B})<br>$$</p>
<p>更一般的，如果对这个$B$做一个划分$B_1,B_2, \dots, B_n$，那么就有</p>
<p>$$<br>P(A)&#x3D;P(AB_1)+P(AB_2)+ \dots + P(AB_n)<br>$$</p>
<p>这个公式称之为全概率公式。</p>
<h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>我们可以推导出事件$A$与$B$同时发生的概率</p>
<p>$$<br>P(AB)&#x3D;P(A|B)P(B)<br>$$</p>
<p>同时也可以被转化为除法法则</p>
<p>$$<br>P(A|B)&#x3D;\frac{P(AB)}{P(B)}<br>$$</p>
<p>如果事件$A,B$是独立的，那么就会有这样的乘法公式</p>
<p>$$<br>P(AB)&#x3D;P(A)P(B)<br>$$</p>
<blockquote>
<p>比如说要检查抛两个硬币的结果是否独立，应该计算它们同时呈现背面的频次，并确认它等于每枚硬币呈现背面的频次的乘积</p>
</blockquote>
<p>而根据上面的公式，我们可以得到极其重要的<strong>贝叶斯公式</strong></p>
<p>$$<br>P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}<br>$$</p>
<p>将全概率公式表示为条件概率的加权和为</p>
<p>$$<br>P(A)&#x3D;P(A|B_1)+P(A|B_2)+ \dots + P(A|B_n)<br>$$</p>
<p>带入贝叶斯公式就有<br>$$<br>P(A|B)&#x3D;\frac{P(B_i|A)[P(A|B_1)+P(A|B_2)+ \dots + P(A|B_n)]}{P(B_i)}<br>$$</p>
<p>使用贝叶斯公式，可以解决著名的三门问题（蒙蒂霍尔问题）</p>
<blockquote>
<p>蒙提霍尔问题（英文：Monty Hall problem），亦称为蒙特霍问题、山羊问题或三门问题，是一个源自博弈论的数学游戏问题，参赛者会看见三扇门，其中一扇门的里面有一辆汽车，选中里面是汽车的那扇门，就可以赢得该辆汽车，另外两扇门里面则都是一只山羊。当参赛者选定了一扇门，主持人会开启另一扇是山羊的门；并问：“要不要换一扇门？”依照玛丽莲·沃斯·莎凡特的见解，参赛者应该换，换门的话，赢得汽车的概率是2&#x2F;3</p>
</blockquote>
<p>在使用贝叶斯公式时，有时候吧$A$称为假设，$B$称为证据。在很多情况下，易知$P(B|A)$，但很难知道$P(A|B)$，即得知证据$B$的情况下$A$发生的概率。</p>
<p>这里，我们假设三个变量$X$是参赛者选择的门，$Y$是后面藏有汽车的门，$Z$是蒙蒂打开的门</p>
<p>$$<br>P(Y&#x3D;B|X&#x3D;A,Z&#x3D;C) &#x3D; \frac{P(X&#x3D;A,Z&#x3D;C|Y&#x3D;B)P(Y&#x3D;B)}{P(X&#x3D;A,Z&#x3D;C)}<br>$$</p>
<p>根据全概率公式</p>
<p>$$<br>\begin{align*}<br>P(X &#x3D; A,Z &#x3D; C) &amp;&#x3D; P(X &#x3D; A,Z &#x3D; C|Y &#x3D; A)P(Y &#x3D; A)+P(X &#x3D; A,Z &#x3D; C|Y &#x3D; B)P(Y &#x3D; B)\\ &amp;+P(X &#x3D; A,Z &#x3D; C|Y &#x3D; C)P(Y &#x3D; C) \\<br>&amp;&#x3D; \frac{1}{2} \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 0 \cdot \frac{1}{3}\\<br>&amp;&#x3D; \frac{1}{2}<br>\end{align*}<br>$$</p>
<p>所以</p>
<p>$$<br>P(Y&#x3D;B|X&#x3D;A,Z&#x3D;C) &#x3D; \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}}&#x3D;\frac{2}{3}<br>$$</p>
<p>而对于不更换的情况，则</p>
<p>$$<br>\begin{align*}<br>P(Y&#x3D;A|X&#x3D;A,Z&#x3D;C) &amp;&#x3D; \frac{P(X&#x3D;A,Z&#x3D;C|Y&#x3D;A)P(Y&#x3D;B)}{P(X&#x3D;A,Z&#x3D;C)}\\<br>&amp;&#x3D; \frac{\frac{1}{2} \cdot \frac{1}{3}}{\frac{1}{2}} \\<br>&amp;&#x3D; \frac{1}{3}<br>\end{align*}<br>$$</p>
<p>我们发现，换门尽然可以更高概率的获奖！其实，三门问题是多门问题之中最难的情况。如果把三门变成千门，参赛者第一次就选中的概率就是1&#x2F;1000，参赛者就会清楚自己完全是猜测，而不是如同三门的时候，1&#x2F;3的概率，所以认为自己是正确的。这样，当主持人打开剩下999扇门中的998扇时，该如何选择，认真思考就会比三门的时候清晰很多（换与不换，中奖的比率是999:1)。</p>
]]></content>
      <categories>
        <category>学术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>因果推断</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning 学习笔记 01</title>
    <url>/2022/10/08/MachineLearning01/</url>
    <content><![CDATA[<p><img src="/img/MachineLearning/cover.png"><br>本博客阅读的书籍是《机器学习》，作者为周志华</p>
<hr>
<p>机器学习所研究的主要内容，是关于在计算机上从数据中产生”模型”的算法，即学习算法（learning algorithm）。而”模型”泛指从数据中学得的结果</p>
<h2 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h2><h3 id="数据集（data-set）"><a href="#数据集（data-set）" class="headerlink" title="数据集（data set）"></a>数据集（data set）</h3><p>一组记录的集合称之为数据集，其中每条记录关于一个事件或对象的描述，称之为示例（instance）或者样本（sample）。</p>
<p>而每条记录中反映事件或对象在某方面的表现或性质的事项称之为属性（attribute）或者特征（feature）。属性的取值称之为属性值（attribute value）。属性张成的空间称之为属性空间（attribute space）或者样本空间（sample space）。</p>
<p>如果我们把每条记录中的属性标记在坐标轴上，那么每个记录其对应的属性值都可以在空间中找到一个点。因此我们也把一个示例称作一个特征向量（feature vector）</p>
<p>数学描述的话，一般地，令$D&#x3D;{\boldsymbol{x_1},\boldsymbol{x_2}, \dots, \boldsymbol{x_m}}$表示包含$m$个示例的数据集，每个示例都有$d$个属性描述，即每个示例$\boldsymbol{x_i}&#x3D;(x_{i1},x_{i2}, \dots, x_{x_{id}})$是$d$维样本空间$\mathcal{X}$中的一个向量，$\boldsymbol{x_i} \in \mathcal{X}$，其中$x_{ij}$是$\boldsymbol{x_i}$在第$j$个属性上的取值，$d$称为样本的维度（dimensionality）</p>
<h3 id="学习（learning）"><a href="#学习（learning）" class="headerlink" title="学习（learning）"></a>学习（learning）</h3><p>从数据中学得模型的过程称之为学习或者训练，这个过程通过执行某种学习算法来完成。学得模型对应了关于数据的某种潜在的规律，因此也称之为假设（hypothesis）</p>
<p>在学习过程中，关于示例结果的信息，称之为标记（label），有标记信息的示例就是样例（example）。</p>
<p>一般地，使用$(\boldsymbol{x_i},y_i)$来表示第$i$个样例，其中$y_i \in \mathcal{Y}$是示例$\boldsymbol{x_i}$的标记，$\mathcal{Y}$是标记的集合，也称作标记空间（label space）。</p>
<h3 id="预测（prediction）"><a href="#预测（prediction）" class="headerlink" title="预测（prediction）"></a>预测（prediction）</h3><p>如果我们预测的是离散值，那么此类学习任务称之为分类（classification）；若预测的是连续值，那么此类学习任务称之为回归（regression）。对只涉及两个类别的二分法（binary classification）任务，通常把其中一个称为正类（positive class），另一个为负类（negative class）；涉及多个分类问题时，就是多分类问题（multi-class classification）</p>
<p>一般地，预测任务 是希望通过对训练集${(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),\dots, (\boldsymbol{x_m},y_m)}$进行学习，建立一个输入空间到输出空间的映射$f: \mathcal{X} \mapsto \mathcal{Y}$。对于二分类任务，通常令$\mathcal{Y}&#x3D;{1,-1}$或者${0,1}$；对于多分类问题，$|\mathcal{Y}|&gt;2$；而对于回归问题，那么$\mathcal{Y}&#x3D;\mathbb{R}$。</p>
<p>学成模型后，我们还可以对其进行测试（testing），被预测的样本称之为（testing sample）</p>
<h3 id="聚类（clustering）"><a href="#聚类（clustering）" class="headerlink" title="聚类（clustering）"></a>聚类（clustering）</h3><p>将训练集中不同记录分成若干组，每组称之为一个簇（cluster）。一般情况下这样的训练集中没有标记信息。</p>
<blockquote>
<p>另外，根据训练集是否有标记信息，可以把学习任务分为两大类：监督学习（supervised learning）和无监督学习（unsupervised learning）。分类和回归是前者的代表，聚类是后者的代表。</p>
</blockquote>
<h3 id="泛化（generalization）"><a href="#泛化（generalization）" class="headerlink" title="泛化（generalization）"></a>泛化（generalization）</h3><p>学得模型适用于新样本的能力称为泛化。具有强大泛化能力的模型能够很好的适应样本空间。通常我们假设样本空间中全体样本的分布服从一个未知分布$\mathcal{D}$，我们获得的每个样本都是独立地从这个分布上采样获得的，即独立同分布（independent and identical distributed，简称i.i.d）。一般而言，训练样本越多，我们得到关于$\mathcal{D}$的信息越多，模型的泛化能力可能会越强。</p>
<blockquote>
<p>实际上，样本空间的规模都是巨大的，我们不可能全部使用一遍用来训练。</p>
</blockquote>
<h2 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h2><h3 id="归纳学习（inductive-learning）"><a href="#归纳学习（inductive-learning）" class="headerlink" title="归纳学习（inductive learning）"></a>归纳学习（inductive learning）</h3><p>“从样例中学习”是归纳学习的过程。</p>
<p>广义的归纳学习大体相当于机器学习，而狭义的归纳学习则要求从训练数据中学得概念，概念学习中最基本的是布尔学习。在这个过程中程序“记住”了样本，但是我们要泛化，所以可以把这个学习过程看作是一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索的目标是找到与训练集匹配（fit）的假设。假设的表示一旦确定，假设空间的规模大小就确定了。</p>
<blockquote>
<p>在现实问题中没有雨假设空间很大，我们时常只是用有限的样本，因此会导致有多个假设与训练集一直，即存在一个和训练集一致的假设集合，称之为版本空间（version space）</p>
</blockquote>
<h3 id="归纳偏好（inductive-bias）"><a href="#归纳偏好（inductive-bias）" class="headerlink" title="归纳偏好（inductive bias）"></a>归纳偏好（inductive bias）</h3><p>针对有限的样本，我们很难断定哪一个假设更好，这时候算法本身的偏好很重要。比如有些时候算法喜欢“特殊”的模型，有些时候算法喜欢“尽可能一般”的模型。</p>
<p><img src="/img/MachineLearning/0000.png"></p>
<p>上图中，如果认为相似样本应有相似的输出，那么就应该是比较平滑的曲线$A$而不是$B$。</p>
<p>一般情况下，我们可以认为“若多个假设与观察一致，则选最简单的那个”。</p>
<h2 id="“没有免费的午餐”定理（No-Free-Lunch-Theorem）"><a href="#“没有免费的午餐”定理（No-Free-Lunch-Theorem）" class="headerlink" title="“没有免费的午餐”定理（No Free Lunch Theorem）"></a>“没有免费的午餐”定理（No Free Lunch Theorem）</h2><p>不同归纳算法之间的误差期望没有谁更优，都是一样的。也就是说我们令$f$为希望学习的真实目标函数，$\mathcal{L}_a, \mathcal{L}_b$是两种不同的学习方法，那么他们的总误差满足：</p>
<p>$$<br>\sum_fE_{ote}(\mathcal{L}_a|X,f)&#x3D;\sum_fE_{ote}(\mathcal{L}_b|X,f)<br>$$</p>
<p>事实上，NFL定理有一个前提，那就是所有问题出现的机会向灯，或者说所有问题同等重要，然后在实际问题中并不是这样的，我们可以在规定问题后找出一个更加优越的算法。</p>
]]></content>
      <categories>
        <category>学术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Adversarial Nets论文阅读笔记</title>
    <url>/2022/09/19/ReciewGAN/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>学术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Cognitive Modelling for Predicting Examinee Performance论文阅读笔记</title>
    <url>/2022/09/19/ReviewCMPEP/</url>
    <content><![CDATA[<p>这部分还在写，挺长的……需要花时间去写文章的内容</p>
<p>不过这个是长时间更新的项目，我希望多花点时间去做这些事情</p>
<p>希望一切安好</p>
]]></content>
      <categories>
        <category>学术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>论文阅读</tag>
        <tag>认知诊断</tag>
      </tags>
  </entry>
  <entry>
    <title>基于glut搭建简易图形学实验框架</title>
    <url>/2022/09/19/openGLFramework/</url>
    <content><![CDATA[<h1 id="基于glut搭建简易图形学实验框架"><a href="#基于glut搭建简易图形学实验框架" class="headerlink" title="基于glut搭建简易图形学实验框架"></a>基于glut搭建简易图形学实验框架</h1><p>框架已经搭建完成，博客的书写需要时间</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>OpenGL</tag>
        <tag>图形学</tag>
      </tags>
  </entry>
  <entry>
    <title>academic</title>
    <url>/academic/index.html</url>
    <content><![CDATA[<p>已有Paper索引：<br>Paper Index:</p>
]]></content>
  </entry>
  <entry>
    <title>about</title>
    <url>/about/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>tags</title>
    <url>/tags/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>search</title>
    <url>/search/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>categories</title>
    <url>/categories/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>gitbook</title>
    <url>/gitbook/index.html</url>
    <content><![CDATA[<p>已有Gitbook索引：<br>Gitbook Index:</p>
<p>2022-08-08: 一本书入门ECNU计算机科学与技术本科</p>
]]></content>
  </entry>
</search>
